[GENERAL]
PORT = 3001 # Port to run the server on
SIMILARITY_MEASURE = "cosine" # "cosine" or "dot"
KEEP_ALIVE = "5m" # How long to keep Ollama models loaded into memory. (Instead of using -1 use "-1m")

[MODELS.OPENAI]
API_KEY = ""

[MODELS.GROQ]
API_KEY = ""

[MODELS.ANTHROPIC]
API_KEY = ""

[MODELS.GEMINI]
API_KEY = ""

[MODELS.DEEPSEEK]
API_KEY = ""
STREAM_DELAY = 5  # Milliseconds between token emissions for reasoning models (higher = slower, 0 = no delay)

[MODELS.OLLAMA]
API_URL = "" # Ollama API URL - http://host.docker.internal:11434

[MODELS.LMSTUDIO]
API_URL = "" # LM STUDIO API URL - http://host.docker.internal:1234

[MODELS.CUSTOM_OPENAI]
API_KEY = ""
API_URL = ""
MODEL_NAME = ""

[API_ENDPOINTS]
SEARXNG = "http://localhost:32768" # SearxNG API URL
